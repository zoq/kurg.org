<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" type="text/css" href="style.css">
    <title>Marcus Edel - kurg.org</title>
</head>

<body>
    <h2><a href="http://kurg.org/">Personal website of Marcus Edel</a></h2>
    <b>Contact: marcus.edel (at) fu-berlin.de</b>
    <p style="text-align:right"><a href="index.html">About</a> <a href="https://github.com/zoq">Github</a> </a><a href="vita.html">CV</a></p>
    <p></p>
    <h3> </h3> I am a student at the <a href="http://www.mi.fu-berlin.de/en/inf/">Institute of Computer Science at Free University of Berlin</a>. My research interests include (modular/hierarchical) <b>reinforcement learning</b>, (stochastic/black-box) <b>optimization methods</b> and (deep/recurrent) <b>neural networks</b>. I have used these techniques in application domains ranging from computer vision and neural decoding to localization and robotics.
    <h3 style="color: #ffff70;">Publications</h3>
    <ul>
        <li>
            <blockquote lang="en">Here is a list of publications, listed in reverse order of publication. I also have a <a href="https://scholar.google.com/citations?user=4PUou_4AAAAJ">Google Scholar profile</a> although the information there may not always be up-to-date.</blockquote>
        </li>
        <li d="Deep Neural Networks for Multimodal Wearable Activity Recognition"><a href="">[l]</a> M. Edel, E. K&ouml;ppe. "Deep Neural Networks for Multimodal Wearable Activity Recognition", in preparation.
        </li>
        <li d="Designing and building the mlpack open-source machine learning library"><a href=pub/pdf/2016cram.pdf>[l]</a> R.R. Curtin, M. Edel. "Designing and building the mlpack open-source machine learning library", <b>submitted to The Fourth International Conference of PUST (ICOPUST)</b>, 2017.
          <br/>
          <br/> Reference: <a href=https://github.com/mlpack/mlpack>[code]</a> <a href=pub/pdf/2017designing.pdf>[pdf]</a>
        </li>
        <li d="Capacity Visual Attention Networks"><a href="pub/pdf/2016cram.pdf">[l]</a> M. Edel, J. Lausch. "Capacity Visual Attention Networks", <b>in Second Global Conference on Artificial Intelligence (GCAI)</b>, Proceeedings (Christoph Benzm&uuml;ller, Raul Rojas, Geoff Sutcliffe, eds.), EasyChair, EPiC Series in Computing, 2016.
            <br/>
            <br/> Reference: <a href="pub/cite/2016cram.bib">[bib]</a> <a href="pub/pdf/2016cram.pdf">[pdf]</a>
        </li>
        <li d="Binarized-BLSTM-RNN based Human Activity Recognition"><a href="pub/pdf/2016bblstm.pdf">[l]</a> M. Edel, E. K&ouml;ppe. "Binarized-BLSTM-RNN based Human Activity Recognition", <b>in Proceedings of the 2016 IEEE International Conference on Indoor Positioning and Indoor Navigation (IPIN)</b>, 2016.
            <br/>
            <br/> Reference: <a href="pub/cite/2016bblstm.bib">[bib]</a> <a href="pub/pdf/2016bblstm.pdf">[pdf]</a>
        </li>
        <li d="An Advanced Method for Pedestrian Dead Reckoning using BLSTM-RNNs"><a href="pub/pdf/2015pdr.pdf">[l]</a> M. Edel, E. K&ouml;ppe. "An Advanced Method for Pedestrian Dead Reckoning using BLSTM-RNNs", <b>in Proceedings of the 2015 IEEE International Conference on Indoor Positioning and Indoor Navigation (IPIN)</b>, 2015.
            <br/>
            <br/> Reference: <a href="pub/cite/2015pdr.bib">[bib]</a> <a href="pub/pdf/2015pdr.pdf">[pdf]</a>
        </li>
        <li d="An automatic benchmarking system"><a href="pub/pdf/2014automatic.pdf">[l]</a> M. Edel, A. Soni, R.R. Curtin. "An automatic benchmarking system", <b>in NIPS 2014 Workshop on Software Engineering for Machine Learning</b>, 2014.
            <br/>
            <br/> Reference: <a href="https://github.com/mlpack/benchmarks">[code]</a> <a href="pub/cite/2014benchmark.bib">[bib]</a> <a href="pub/pdf/2014automatic.pdf">[pdf]</a>
        </li>
    </ul>
    <h3 style="color: #ffff70;">Code &amp; Demos</h3>
    <ul>
        <li>
            <blockquote lang="en">In order to make my research as reproducible as possible, most of the algorithms and benchmarks I use are available open-source through the <a href="http://www.mlpack.org">mlpack machine learning library</a>. Here is a list of demos, that entirely run in your browser. No software requirements, no compilers, no installations, no GPUs.</blockquote>
        </li>
        <li><a href="http://kurg.org/mnist/index.html">[l]</a> Classify MNIST digits with a Convolutional Neural Network <a href="http://kurg.org/mnist/index.html">[demo]</a> <a href="https://www.github.com/zoq/nn-demos">[code]</a></li>
        <li><a href="http://kurg.org/rmva/index.html">[l]</a> Classify MNIST digits with Recurrent Model of Visual Attention <a href="http://kurg.org/rmva/index.html">[demo]</a> <a href="https://www.github.com/zoq/nn-demos">[code]</a></li>
        <li><a href="http://www.mlpack.org/benchmarks.html">[l]</a> An automatic benchmarking system <a href="http://www.mlpack.org/benchmarks.html">[interface]</a> <a href="https://github.com/mlpack/benchmarks">[code]</a></li>
    </ul>
    <p></p>
</body>
<div></div>

</html>
